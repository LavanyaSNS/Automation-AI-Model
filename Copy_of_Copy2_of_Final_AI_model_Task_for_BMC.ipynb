{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1TLZKxBmNsX2IrTWMn3OAO2zrniSlEij5",
      "authorship_tag": "ABX9TyNCoi8MiHVjceDe0yyL/6vE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LavanyaSNS/Automation-AI-Model/blob/main/Copy_of_Copy2_of_Final_AI_model_Task_for_BMC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install yt-dlp whisper transformers accelerate torch pytesseract pydub webrtcvad easyocr groq\n",
        "\n",
        "# Download YOLOv3 weights and configuration files\n",
        "!wget https://github.com/pjreddie/darknet/blob/master/cfg/yolov3.cfg -O yolov3.cfg\n",
        "!wget https://github.com/pjreddie/darknet/blob/master/yolov3.weights -O yolov3.weights\n",
        "\n",
        "# Install ffmpeg\n",
        "!apt-get install -y ffmpeg\n",
        "\n",
        "# Check ffmpeg version\n",
        "!ffmpeg -version\n",
        "\n",
        "!pip install git+https://github.com/openai/whisper.git"
      ],
      "metadata": {
        "id": "K3vg2T33F9bu",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1: Download and Validate YouTube Video and Audio"
      ],
      "metadata": {
        "id": "ofuCdniWrzQJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Video Information Retrieval: Getting the title of the video from the URL.\n",
        "2. Video Downloading: Downloading the video in a specified format.\n",
        "3. Video Quality Checking: Verifying the resolution of the video.\n",
        "4. Audio Extraction: Extracting audio from the downloaded video.\n",
        "5. Audio Quality Checks: Ensuring the audio meets quality standards (bitrate, channels, duration).\n",
        "6. Tone and Noise Checks: Analyzing the audio for pitch and background noise.\n",
        "7. Voice Presence Detection: Using VAD to check if human speech is present in the audio.\n",
        "8. Audio Transcription: Converting the audio to text using Whisper."
      ],
      "metadata": {
        "id": "0DjAAEVybAvT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import yt_dlp\n",
        "import subprocess\n",
        "import whisper\n",
        "from pydub import AudioSegment\n",
        "import librosa\n",
        "import webrtcvad\n",
        "import numpy as np\n",
        "import wave\n",
        "import contextlib\n",
        "\n",
        "def get_video_info(video_url):\n",
        "    ydl_opts = {'skip_download': True}\n",
        "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "        info = ydl.extract_info(video_url, download=False)\n",
        "        title = info.get('title', 'No title found')\n",
        "        return title\n",
        "\n",
        "def download_video(video_url, output_path=\"video.mp4.mkv\"):\n",
        "    ydl_opts = {\n",
        "        'format': 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/mp4',\n",
        "        'outtmpl': output_path,\n",
        "        'merge_output_format': 'mkv',\n",
        "    }\n",
        "    try:\n",
        "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "            ydl.download([video_url])\n",
        "        return output_path\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading video: {e}\")\n",
        "        return None\n",
        "\n",
        "def check_video_quality(video_path):\n",
        "    command = [\n",
        "        'ffprobe',\n",
        "        '-v', 'error',\n",
        "        '-select_streams', 'v:0',\n",
        "        '-show_entries', 'stream=width,height',\n",
        "        '-of', 'csv=p=0',\n",
        "        video_path\n",
        "    ]\n",
        "    try:\n",
        "        output = subprocess.check_output(command).decode().strip()\n",
        "        width, height = map(int, output.split(','))\n",
        "\n",
        "        if height < 480:\n",
        "            return False, f\"Video quality issue: The video resolution is {width}x{height}, but it must be at least 480p.\"\n",
        "        return True, f\"Video is valid: {width}x{height} resolution.\"\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        return False, f\"Error checking video quality: {e}\"\n",
        "\n",
        "def extract_audio(video_path, audio_output_path=\"audio.mp3\"):\n",
        "    command = [\n",
        "        'ffmpeg',\n",
        "        '-i', video_path,\n",
        "        '-q:a', '0',\n",
        "        '-map', 'a',\n",
        "        audio_output_path\n",
        "    ]\n",
        "    try:\n",
        "        subprocess.run(command, check=True, stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n",
        "        return audio_output_path\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error extracting audio: {e.stderr.decode()}\")  # Print error message from ffmpeg\n",
        "        return None\n",
        "\n",
        "\n",
        "# Audio Quality, Tone, Background Noise Checks\n",
        "\n",
        "def check_audio_quality(audio_path):\n",
        "    audio = AudioSegment.from_mp3(audio_path)\n",
        "    bitrate = audio.frame_rate\n",
        "    channels = audio.channels\n",
        "    duration = len(audio) / 1000  # Duration in seconds\n",
        "\n",
        "    if bitrate < 16000:  # Sample rate should be at least 16kHz\n",
        "        return False, \"Audio quality too low (bitrate < 16kHz).\"\n",
        "    if channels != 1:\n",
        "        # Convert to mono if it has multiple channels\n",
        "        audio = audio.set_channels(1)\n",
        "        audio.export(audio_path, format='mp3')  # Overwrite the original audio\n",
        "        channels = 1  # Update the channels count\n",
        "\n",
        "    if duration < 240:  # Check if the audio is less than 4 minutes (240 seconds)\n",
        "        return False, \"Audio file too short (less than 4 minutes).\"\n",
        "\n",
        "    return True, \"Audio quality is acceptable.\"\n",
        "\n",
        "\n",
        "\n",
        "def check_audio_tone(audio_path):\n",
        "    audio_data, sr = librosa.load(audio_path)\n",
        "    pitches, magnitudes = librosa.piptrack(y=audio_data, sr=sr)\n",
        "\n",
        "    # Calculate the mean pitch\n",
        "    pitch_values = [pitches[i][pitches[i] > 0] for i in range(len(pitches))]\n",
        "    pitch_mean = [np.mean(pitch) if len(pitch) > 0 else 0 for pitch in pitch_values]\n",
        "\n",
        "    if np.mean(pitch_mean) < 75:  # Too low, possibly not human speech\n",
        "        return False, \"Audio tone may be too low for valid speech.\"\n",
        "    return True, \"Audio tone is acceptable.\"\n",
        "\n",
        "\n",
        "def check_background_noise(audio_path):\n",
        "    audio_data, sr = librosa.load(audio_path)\n",
        "    energy = np.sum(audio_data ** 2) / len(audio_data)\n",
        "    noise_threshold = np.percentile(np.abs(audio_data), 5)\n",
        "    noise_energy = np.mean(np.abs(audio_data)[np.abs(audio_data) < noise_threshold] ** 2)\n",
        "\n",
        "    snr = 10 * np.log10(energy / noise_energy)\n",
        "\n",
        "    if snr < 15:  # Minimum SNR of 15dB\n",
        "        return False, \"Too much background noise.\"\n",
        "    return True, \"Background noise level is acceptable.\"\n",
        "\n",
        "\n",
        "def convert_to_pcm(audio_path):\n",
        "    \"\"\"\n",
        "    Convert audio to 16-bit mono PCM with a sample rate of 16kHz.\n",
        "    \"\"\"\n",
        "    sound = AudioSegment.from_file(audio_path)\n",
        "    sound = sound.set_channels(1)  # Convert to mono\n",
        "    sound = sound.set_frame_rate(16000)  # Set sample rate to 16kHz\n",
        "    pcm_path = \"converted_audio.wav\"\n",
        "    sound.export(pcm_path, format=\"wav\")\n",
        "    return pcm_path\n",
        "\n",
        "\n",
        "def check_voice_presence(audio_path):\n",
        "    \"\"\"\n",
        "    Check if the audio contains human speech using webrtcvad.\n",
        "    \"\"\"\n",
        "    pcm_audio_path = convert_to_pcm(audio_path)\n",
        "\n",
        "    vad = webrtcvad.Vad()\n",
        "    vad.set_mode(1)  # Mode 1: more aggressive detection of speech\n",
        "\n",
        "    with contextlib.closing(wave.open(pcm_audio_path, 'rb')) as wf:\n",
        "        sample_rate = wf.getframerate()\n",
        "        assert sample_rate == 16000, \"Sample rate must be 16000 Hz\"\n",
        "        pcm_data = wf.readframes(wf.getnframes())\n",
        "\n",
        "        frame_duration = 30  # Frame size: 30ms\n",
        "        frame_size = int(sample_rate * frame_duration / 1000)\n",
        "\n",
        "        for i in range(0, len(pcm_data), frame_size * 2):  # 2 bytes per sample (16-bit audio)\n",
        "            frame = pcm_data[i:i + frame_size * 2]\n",
        "            if vad.is_speech(frame, sample_rate):\n",
        "                return True, \"Voice detected in the audio.\"\n",
        "\n",
        "    return False, \"No voice detected in the audio.\"\n",
        "\n",
        "\n",
        "def transcribe_audio(audio_path):\n",
        "    model = whisper.load_model(\"small\")\n",
        "    result = model.transcribe(audio_path)\n",
        "\n",
        "    transcription_text_file = \"/content/transcription.txt\"\n",
        "    transcription_text = result['text']\n",
        "    with open(transcription_text_file, 'w') as f:\n",
        "        f.write(transcription_text)\n",
        "\n",
        "    print(\"Transcription saved to transcription.txt\")\n",
        "    return transcription_text_file\n",
        "\n",
        "# Example Usage\n",
        "video_url = 'https://youtu.be/54keG0E81xQ?si=2saiQTvV2gtWcDy2'\n",
        "title = get_video_info(video_url)\n",
        "video_file = download_video(video_url)\n",
        "\n",
        "if video_file:\n",
        "    print(f\"Video downloaded successfully: {video_file}\")\n",
        "    print(f\"Video title: {title}\")\n",
        "\n",
        "    quality_check, quality_report = check_video_quality(video_file)\n",
        "    print(quality_report)\n",
        "\n",
        "    if quality_check:  # Proceed to extract audio only if the video quality is satisfied\n",
        "        audio_file = extract_audio(video_file)\n",
        "        if audio_file:  # Check if audio was extracted successfully\n",
        "            print(f\"Audio extracted successfully: {audio_file}\")\n",
        "\n",
        "            audio_checks = [\n",
        "                check_audio_quality(audio_file),  # Returns (True/False, \"message\")\n",
        "                check_audio_tone(audio_file),     # Returns (True/False, \"message\")\n",
        "                check_background_noise(audio_file),# Returns (True/False, \"message\")\n",
        "                check_voice_presence(audio_file)  # Returns (True/False, \"message\")\n",
        "            ]\n",
        "\n",
        "            # Check if all conditions are satisfied\n",
        "            if all(check[0] for check in audio_checks):  # Each `check` is a tuple now\n",
        "                transcription_file = transcribe_audio(audio_file)\n",
        "                print(f\"Transcription saved to: {transcription_file}\")\n",
        "            else:\n",
        "                # If any condition fails, print the corresponding message\n",
        "                for check in audio_checks:\n",
        "                    if not check[0]:  # If the check failed\n",
        "                        print(check[1])  # Print the associated message\n",
        "\n",
        "        else:\n",
        "            print(\"Audio extraction failed.\")\n",
        "    else:\n",
        "        print(\"Audio extraction skipped due to low video quality.\")\n",
        "else:\n",
        "    print(\"Failed to download the video.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "nuYgSqtDecWg",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Frame Checking"
      ],
      "metadata": {
        "id": "pI45EV3t3vWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "def extract_images_from_video(video_path, output_dir=\"frames\", interval=10):\n",
        "    \"\"\"\n",
        "    Extract frames from video every 'interval' seconds.\n",
        "    :param video_path: Path to the video file.\n",
        "    :param output_dir: Directory to save the extracted images.\n",
        "    :param interval: Time interval between frames to extract (in seconds).\n",
        "    \"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)  # Frames per second\n",
        "    frame_interval = int(fps * interval)\n",
        "\n",
        "    count = 0\n",
        "    success = True\n",
        "    while success:\n",
        "        success, frame = cap.read()\n",
        "        if count % frame_interval == 0 and success:\n",
        "            # Use zero-padding to ensure correct sorting\n",
        "            img_path = os.path.join(output_dir, f\"frame_{count:05d}.jpg\")\n",
        "            cv2.imwrite(img_path, frame)\n",
        "        count += 1\n",
        "\n",
        "    cap.release()\n",
        "    print(f\"Images extracted to {output_dir}\")\n",
        "\n",
        "# Example usage\n",
        "extract_images_from_video('/content/video.mp4.mkv', interval=15)"
      ],
      "metadata": {
        "id": "lmY72GXANLvq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0eb6fe4-60dd-4360-a995-114e07cac2bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Images extracted to frames\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Functions for Text Extraction and Uniqueness Check"
      ],
      "metadata": {
        "id": "OMBd-MB2va6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import easyocr\n",
        "import whisper\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "import numpy as np\n",
        "\n",
        "def is_unique_image(img_path, unique_images, similarity_threshold=0.95):\n",
        "    \"\"\"\n",
        "    Check if the image is unique compared to already processed unique images.\n",
        "    :param img_path: Path to the image file.\n",
        "    :param unique_images: List of already processed unique images.\n",
        "    :param similarity_threshold: Similarity threshold for uniqueness.\n",
        "    :return: True if the image is unique, False otherwise.\n",
        "    \"\"\"\n",
        "    img = cv2.imread(img_path)\n",
        "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    for unique_img in unique_images:\n",
        "        unique_img_gray = cv2.cvtColor(unique_img, cv2.COLOR_BGR2GRAY)\n",
        "        # Calculate the Structural Similarity Index (SSI)\n",
        "        score = ssim(img_gray, unique_img_gray)\n",
        "        if score >= similarity_threshold:\n",
        "            return False  # Image is similar to an already processed unique image\n",
        "\n",
        "    return True  # Image is unique\n",
        "\n",
        "'''def extract_text_from_image(img_path):\n",
        "    \"\"\"\n",
        "    Extract text from an image using EasyOCR.\n",
        "    :param img_path: Path to the image file.\n",
        "    :return: Extracted text.\n",
        "    \"\"\"\n",
        "    reader = easyocr.Reader(['en'])\n",
        "    results = reader.readtext(img_path)\n",
        "    extracted_text = \" \".join([text[1] for text in results])  # Concatenate all detected text\n",
        "    return extracted_text'''\n",
        "\n",
        "def extract_text_from_image(img_path):\n",
        "    \"\"\"\n",
        "    Extract text from an image using EasyOCR. Skip if no text is detected.\n",
        "    :param img_path: Path to the image file.\n",
        "    :return: Extracted text or None if no text is detected.\n",
        "    \"\"\"\n",
        "    reader = easyocr.Reader(['en'])\n",
        "    results = reader.readtext(img_path)\n",
        "\n",
        "    if not results:  # No text detected\n",
        "        print(f\"No text detected in image: {img_path}\")\n",
        "        return None\n",
        "\n",
        "    extracted_text = \" \".join([text[1] for text in results])  # Concatenate all detected text\n",
        "    return extracted_text\n",
        "\n",
        "def analyze_frames(frames_dir, similarity_threshold=0.95):\n",
        "    \"\"\"\n",
        "    Analyze frames in a directory to extract text from unique images.\n",
        "    :param frames_dir: Directory containing the frames.\n",
        "    :param similarity_threshold: Threshold for image similarity.\n",
        "    :return: Dictionary of extracted texts from unique images.\n",
        "    \"\"\"\n",
        "    unique_images = []\n",
        "    extracted_texts = {}\n",
        "\n",
        "    for image_file in os.listdir(frames_dir):\n",
        "        img_path = os.path.join(frames_dir, image_file)\n",
        "        if is_unique_image(img_path, unique_images, similarity_threshold):\n",
        "            # If unique, read the image and extract text\n",
        "            extracted_text = extract_text_from_image(img_path)\n",
        "            extracted_texts[image_file] = extracted_text\n",
        "            # Store the image for future uniqueness checks\n",
        "            unique_images.append(cv2.imread(img_path))\n",
        "\n",
        "    return extracted_texts\n",
        "\n",
        "def transcribe_audio(audio_path):\n",
        "    \"\"\"\n",
        "    Transcribe audio using Whisper model.\n",
        "    :param audio_path: Path to the audio file.\n",
        "    :return: Transcribed text.\n",
        "    \"\"\"\n",
        "    model = whisper.load_model(\"small\")\n",
        "    result = model.transcribe(audio_path)\n",
        "    return result['text']\n",
        "\n",
        "# Paths\n",
        "frames_directory = '/content/frames'  # Path to the frames directory\n",
        "audio_file_path = '/content/audio.mp3'  # Path to the audio file\n",
        "\n",
        "# Extract text from frames and transcribe audio\n",
        "extracted_texts = analyze_frames(frames_directory)\n",
        "audio_text = transcribe_audio(audio_file_path)\n",
        "\n",
        "# Combine texts\n",
        "combined_text = \"Extracted Texts from Frames:\\n\"\n",
        "for img_name, text in extracted_texts.items():\n",
        "    combined_text += f\"Extracted Text from {img_name}:\\n{text}\\n\\n\"\n",
        "\n",
        "combined_text += \"Transcribed Audio Text:\\n\"\n",
        "combined_text += audio_text\n",
        "\n",
        "# Save to a text file\n",
        "output_file_path = '/content/extracted_texts_combined.txt'\n",
        "with open(output_file_path, 'w') as file:\n",
        "    file.write(combined_text)\n",
        "\n",
        "print(f\"Combined extracted texts saved to {output_file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Euo_RPWvQQ0",
        "outputId": "da74d859-fe8a-48a8-a72d-eb9d2e2a4b8a",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:easyocr.easyocr:Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
            "WARNING:easyocr.easyocr:Downloading detection model, please wait. This may take several minutes depending upon your network connection.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress: |██████████████████████████████████████████████████| 100.0% Complete"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:easyocr.easyocr:Downloading recognition model, please wait. This may take several minutes depending upon your network connection.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress: |██████████████████████████████████████████████████| 100.0% Complete"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:easyocr.easyocr:Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No text detected in image: /content/frames/frame_04950.jpg\n",
            "No text detected in image: /content/frames/frame_00900.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:easyocr.easyocr:Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No text detected in image: /content/frames/frame_03600.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:easyocr.easyocr:Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No text detected in image: /content/frames/frame_01350.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:easyocr.easyocr:Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No text detected in image: /content/frames/frame_05400.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:easyocr.easyocr:Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No text detected in image: /content/frames/frame_03150.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:easyocr.easyocr:Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No text detected in image: /content/frames/frame_01800.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:easyocr.easyocr:Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No text detected in image: /content/frames/frame_02700.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:easyocr.easyocr:Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No text detected in image: /content/frames/frame_00000.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:easyocr.easyocr:Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No text detected in image: /content/frames/frame_02250.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:easyocr.easyocr:Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
            "WARNING:easyocr.easyocr:Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No text detected in image: /content/frames/frame_00450.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:easyocr.easyocr:Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No text detected in image: /content/frames/frame_06300.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:easyocr.easyocr:Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
            "WARNING:easyocr.easyocr:Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No text detected in image: /content/frames/frame_05850.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 461M/461M [00:07<00:00, 61.1MiB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n",
            "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined extracted texts saved to /content/extracted_texts_combined.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Verification"
      ],
      "metadata": {
        "id": "gz-8ufPomMTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "# Set the TogetherAI API key\n",
        "api_key = \"0b79e2f0ddb16654bc98df9f828e0474d53c7d00eac41328abf06bd4858d14bb\"\n",
        "\n",
        "# Load the transcribed text from the file\n",
        "with open(\"/content/extracted_texts_combined.txt\", \"r\") as file:\n",
        "    transcript = file.read()\n",
        "\n",
        "# Set the video title\n",
        "title = \"CONVENTIONAL SCREW\"\n",
        "\n",
        "# Create a more detailed and structured prompt for verification task\n",
        "prompt = f\"\"\"\n",
        "You are tasked with analyzing the transcription of a business model canvas (BMC) presentation video.\n",
        "Please perform the following checks and provide the results in JSON format:\n",
        "\n",
        "1. **Names:** Extract the names of students mentioned when they introduce themselves.\n",
        "2. **Introduction:** Check if there is a formal introduction at the beginning of the transcription. Indicate 'Yes' if an introduction is present and 'No' if not.\n",
        "3. **Relevance:** Verify if the content of the transcription is relevant to the video title '{title}'. Answer 'Yes' or 'No' and explain briefly why the content is or is not relevant.\n",
        "4. **BMC Topics:** Confirm whether the transcription covers all 9 BMC topics: key partners, key activities, value propositions, customer relationships, customer segments, key resources, channels, cost structure, and revenue streams. Answer 'Yes' or 'No' and list the missing topics if any.\n",
        "\n",
        "Transcription to analyze:\n",
        "{transcript}\n",
        "\n",
        "Please provide your analysis in the following JSON format:\n",
        "\n",
        "{{\n",
        "    \"Names\": [\"List of names\"],\n",
        "    \"Introduction\": \"Yes/No\",\n",
        "    \"Relevance\": {{\n",
        "        \"IsRelevant\": \"Yes/No\",\n",
        "        \"Reason\": \"Brief explanation of relevance\"\n",
        "    }},\n",
        "    \"BMC Topics\": {{\n",
        "        \"AllTopicsCovered\": \"Yes/No\",\n",
        "        \"MissingTopics\": [\"List of missing topics, if any\"]\n",
        "    }}\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "# API endpoint\n",
        "url = \"https://api.together.xyz/v1/chat/completions\"\n",
        "\n",
        "# Headers\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {api_key}\",\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "# Request payload\n",
        "data = {\n",
        "    \"model\": \"meta-llama/Llama-Vision-Free\",\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
        "}\n",
        "\n",
        "# Send the request to TogetherAI API\n",
        "response = requests.post(url, headers=headers, data=json.dumps(data))\n",
        "\n",
        "# Check if the request was successful\n",
        "if response.status_code == 200:\n",
        "    # Parse the response\n",
        "    response_content = response.json()\n",
        "    # Extract the relevant message from the response\n",
        "    message_content = response_content[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "    # Print the raw response\n",
        "    print(\"Raw Response:\", message_content)\n",
        "\n",
        "    # Try to parse it as JSON\n",
        "    try:\n",
        "        response_data = json.loads(message_content)  # Load the response as JSON\n",
        "        print(\"Parsed Response:\", json.dumps(response_data, indent=2))  # Pretty print the parsed JSON\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(\"Failed to decode JSON response:\", e)\n",
        "else:\n",
        "    print(f\"Request failed with status code {response.status_code}: {response.text}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98c68868-3eb8-495a-ba2d-b7dfe83bcc58",
        "id": "UDB5U7egatBr"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw Response: Here is the analysis in JSON format:\n",
            "\n",
            "```json\n",
            "{\n",
            "    \"Names\": [\"Steira\", \"Sandesh\", \"Prasimha Bairdhi\", \"Santakumar\"],\n",
            "    \"Introduction\": \"Yes\",\n",
            "    \"Relevance\": {\n",
            "        \"IsRelevant\": \"No\",\n",
            "        \"Reason\": \"The content of the transcription is about a project to design a conventional screw with improved functionality and rust prevention, which is not directly related to the video title 'CONVENTIONAL SCREW'.\"\n",
            "    },\n",
            "    \"BMC Topics\": {\n",
            "        \"AllTopicsCovered\": \"No\",\n",
            "        \"MissingTopics\": [\"Channels\", \"Cost structure\", \"Revenue streams\"]\n",
            "    }\n",
            "}\n",
            "```\n",
            "\n",
            "In the Names section, I extracted the names of the students mentioned in the transcription.\n",
            "\n",
            "For the Introduction section, I marked it as \"Yes\" because there is a brief introduction at the beginning of the transcription.\n",
            "\n",
            "For the Relevance section, I marked it as \"No\" because the content of the transcription is about a project to design a conventional screw, which is not directly related to the video title 'CONVENTIONAL SCREW'. The reason is briefly explained.\n",
            "\n",
            "For the BMC Topics section, I marked it as \"No\" because not all 9 BMC topics are covered in the transcription. The missing topics are listed as \"Channels\", \"Cost structure\", and \"Revenue streams\".\n",
            "Failed to decode JSON response: Expecting value: line 1 column 1 (char 0)\n"
          ]
        }
      ]
    }
  ]
}