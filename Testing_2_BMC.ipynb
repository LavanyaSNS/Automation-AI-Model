{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPn3v6pFojSyyaKVyZcp82v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LavanyaSNS/Automation-AI-Model/blob/main/Testing_2_BMC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install yt-dlp whisper transformers accelerate torch pytesseract pydub webrtcvad easyocr groq\n",
        "\n",
        "# Download YOLOv3 weights and configuration files\n",
        "!wget https://github.com/pjreddie/darknet/blob/master/cfg/yolov3.cfg -O yolov3.cfg\n",
        "!wget https://github.com/pjreddie/darknet/blob/master/yolov3.weights -O yolov3.weights\n",
        "\n",
        "# Install ffmpeg\n",
        "!apt-get install -y ffmpeg\n",
        "\n",
        "# Check ffmpeg version\n",
        "!ffmpeg -version\n",
        "\n",
        "!pip install git+https://github.com/openai/whisper.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27gNDseQToYh",
        "outputId": "a8c74305-9b23-447a-fcae-b643f284db5f",
        "collapsed": true
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yt-dlp in /usr/local/lib/python3.10/dist-packages (2024.10.7)\n",
            "Requirement already satisfied: whisper in /usr/local/lib/python3.10/dist-packages (1.1.10)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.34.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.10/dist-packages (0.3.13)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (0.25.1)\n",
            "Requirement already satisfied: webrtcvad in /usr/local/lib/python3.10/dist-packages (2.0.10)\n",
            "Requirement already satisfied: easyocr in /usr/local/lib/python3.10/dist-packages (1.7.2)\n",
            "Requirement already satisfied: groq in /usr/local/lib/python3.10/dist-packages (0.11.0)\n",
            "Requirement already satisfied: brotli in /usr/local/lib/python3.10/dist-packages (from yt-dlp) (1.1.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from yt-dlp) (2024.8.30)\n",
            "Requirement already satisfied: mutagen in /usr/local/lib/python3.10/dist-packages (from yt-dlp) (1.47.0)\n",
            "Requirement already satisfied: pycryptodomex in /usr/local/lib/python3.10/dist-packages (from yt-dlp) (3.21.0)\n",
            "Requirement already satisfied: requests<3,>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from yt-dlp) (2.32.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.17 in /usr/local/lib/python3.10/dist-packages (from yt-dlp) (2.2.3)\n",
            "Requirement already satisfied: websockets>=13.0 in /usr/local/lib/python3.10/dist-packages (from yt-dlp) (13.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from whisper) (1.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (10.4.0)\n",
            "Requirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.10/dist-packages (from easyocr) (0.19.1+cu121)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from easyocr) (4.10.0.84)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from easyocr) (1.13.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from easyocr) (0.24.0)\n",
            "Requirement already satisfied: python-bidi in /usr/local/lib/python3.10/dist-packages (from easyocr) (0.6.3)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.10/dist-packages (from easyocr) (2.0.6)\n",
            "Requirement already satisfied: pyclipper in /usr/local/lib/python3.10/dist-packages (from easyocr) (1.3.0.post5)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from easyocr) (1.11.1.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from groq) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from groq) (0.27.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (1.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.32.2->yt-dlp) (3.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.1)\n",
            "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (2.35.1)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (2024.9.20)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (0.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "--2024-10-18 09:15:41--  https://github.com/pjreddie/darknet/blob/master/cfg/yolov3.cfg\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘yolov3.cfg’\n",
            "\n",
            "yolov3.cfg              [ <=>                ] 614.62K  --.-KB/s    in 0.07s   \n",
            "\n",
            "2024-10-18 09:15:42 (8.04 MB/s) - ‘yolov3.cfg’ saved [629366]\n",
            "\n",
            "--2024-10-18 09:15:42--  https://github.com/pjreddie/darknet/blob/master/yolov3.weights\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2024-10-18 09:15:42 ERROR 404: Not Found.\n",
            "\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
            "built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
            "configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "libavutil      56. 70.100 / 56. 70.100\n",
            "libavcodec     58.134.100 / 58.134.100\n",
            "libavformat    58. 76.100 / 58. 76.100\n",
            "libavdevice    58. 13.100 / 58. 13.100\n",
            "libavfilter     7.110.100 /  7.110.100\n",
            "libswscale      5.  9.100 /  5.  9.100\n",
            "libswresample   3.  9.100 /  3.  9.100\n",
            "libpostproc    55.  9.100 / 55.  9.100\n",
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-b1or625e\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-b1or625e\n",
            "  Resolved https://github.com/openai/whisper.git to commit 25639fc17ddc013d56c594bfbf7644f2185fad84\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (2.4.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (4.66.5)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (10.5.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (0.8.0)\n",
            "Requirement already satisfied: triton>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton>=2.0.0->openai-whisper==20240930) (3.16.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20240930) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20240930) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20240930) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (3.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (2024.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20240930) (3.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20240930) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "joPSiB8STHf0"
      },
      "outputs": [],
      "source": [
        "import yt_dlp\n",
        "import subprocess\n",
        "\n",
        "def get_video_info(video_url):\n",
        "    ydl_opts = {'skip_download': True}\n",
        "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "        info = ydl.extract_info(video_url, download=False)\n",
        "        title = info.get('title', 'No title found')\n",
        "        # Remove the specified substrings\n",
        "        title = title.replace('SNS COLLEGE OF TECHNOLOGY', '')\n",
        "        title = title.replace('BMC VIDEO', '')\n",
        "        title = title.replace('DT PROJECT', '')\n",
        "\n",
        "        # Optionally, strip any extra spaces or separators (e.g., '|')\n",
        "        title = title.replace('|', '').strip().lower()\n",
        "        return title\n",
        "\n",
        "def download_video(video_url, output_path=\"video.mp4.mkv\"):\n",
        "    ydl_opts = {\n",
        "        'format': 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/mp4',\n",
        "        'outtmpl': output_path,\n",
        "        'merge_output_format': 'mkv',\n",
        "    }\n",
        "    try:\n",
        "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "            ydl.download([video_url])\n",
        "        return output_path\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading video: {e}\")\n",
        "        return None\n",
        "\n",
        "def check_video_quality(video_path):\n",
        "    command = [\n",
        "        'ffprobe',\n",
        "        '-v', 'error',\n",
        "        '-select_streams', 'v:0',\n",
        "        '-show_entries', 'stream=width,height',\n",
        "        '-of', 'csv=p=0',\n",
        "        video_path\n",
        "    ]\n",
        "    try:\n",
        "        output = subprocess.check_output(command).decode().strip()\n",
        "        width, height = map(int, output.split(','))\n",
        "\n",
        "        if height < 480:\n",
        "            return False, f\"Video quality issue: The video resolution is {width}x{height}, but it must be at least 480p.\"\n",
        "        return True, f\"Video is valid: {width}x{height} resolution.\"\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        return False, f\"Error checking video quality: {e}\"\n",
        "\n",
        "'''def extract_audio(video_path, audio_output_path=\"audio.mp3\"):\n",
        "    command = [\n",
        "        'ffmpeg',\n",
        "        '-i', video_path,\n",
        "        '-q:a', '0',\n",
        "        '-map', 'a',\n",
        "        audio_output_path\n",
        "    ]\n",
        "    try:\n",
        "        subprocess.run(command, check=True, stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n",
        "        return audio_output_path\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error extracting audio: {e.stderr.decode()}\")  # Print error message from ffmpeg\n",
        "        return None'''\n",
        "\n",
        "def extract_audio(video_path, audio_output_path=\"audio.mp3\"):\n",
        "    # Check if ffmpeg is installed\n",
        "    if subprocess.call([\"which\", \"ffmpeg\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE) != 0:\n",
        "        print(\"ffmpeg is not installed. Please install it to extract audio.\")\n",
        "        return None\n",
        "\n",
        "    # Check if output file already exists\n",
        "    if os.path.exists(audio_output_path):\n",
        "        overwrite = input(f\"{audio_output_path} already exists. Overwrite? (y/n): \")\n",
        "        if overwrite.lower() != 'y':\n",
        "            print(\"Audio extraction skipped.\")\n",
        "            return audio_output_path  # Return the existing file path\n",
        "\n",
        "    # Prepare the ffmpeg command to extract audio\n",
        "    command = [\n",
        "        'ffmpeg',\n",
        "        '-i', video_path,\n",
        "        '-q:a', '0',\n",
        "        '-map', 'a',\n",
        "        audio_output_path\n",
        "    ]\n",
        "\n",
        "    # Execute the command\n",
        "    try:\n",
        "        print(f\"Executing command: {' '.join(command)}\")  # Debug: Print the command\n",
        "        subprocess.run(command, check=True, stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n",
        "        print(f\"Audio extracted successfully: {audio_output_path}\")\n",
        "        return audio_output_path\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error extracting audio: {e.stderr.decode()}\")  # Print error message from ffmpeg\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "from pydub import AudioSegment\n",
        "import librosa\n",
        "import numpy as np\n",
        "import webrtcvad\n",
        "import wave\n",
        "import contextlib\n",
        "\n",
        "# Audio Quality, Tone, Background Noise Checks\n",
        "def check_audio_quality(audio_path):\n",
        "    audio = AudioSegment.from_mp3(audio_path)\n",
        "    bitrate = audio.frame_rate\n",
        "    channels = audio.channels\n",
        "    duration = len(audio) / 1000  # Duration in seconds\n",
        "\n",
        "    if bitrate < 16000:  # Sample rate should be at least 16kHz\n",
        "        return False, \"Audio quality too low (bitrate < 16kHz).\"\n",
        "    if channels != 1:\n",
        "        # Convert to mono if it has multiple channels\n",
        "        audio = audio.set_channels(1)\n",
        "        audio.export(audio_path, format='mp3')  # Overwrite the original audio\n",
        "        channels = 1  # Update the channels count\n",
        "\n",
        "    if duration < 180:  # Check if the audio is less than 4 minutes (240 seconds)\n",
        "        return False, \"Audio file too short (less than 4 minutes).\"\n",
        "\n",
        "    return True, \"Audio quality is acceptable.\"\n",
        "\n",
        "def check_audio_tone(audio_path):\n",
        "    audio_data, sr = librosa.load(audio_path)\n",
        "    pitches, magnitudes = librosa.piptrack(y=audio_data, sr=sr)\n",
        "\n",
        "    # Calculate the mean pitch\n",
        "    pitch_values = [pitches[i][pitches[i] > 0] for i in range(len(pitches))]\n",
        "    pitch_mean = [np.mean(pitch) if len(pitch) > 0 else 0 for pitch in pitch_values]\n",
        "\n",
        "    if np.mean(pitch_mean) < 75:  # Too low, possibly not human speech\n",
        "        return False, \"Audio tone may be too low for valid speech.\"\n",
        "    return True, \"Audio tone is acceptable.\"\n",
        "\n",
        "\n",
        "def check_background_noise(audio_path):\n",
        "    audio_data, sr = librosa.load(audio_path)\n",
        "    energy = np.sum(audio_data ** 2) / len(audio_data)\n",
        "    noise_threshold = np.percentile(np.abs(audio_data), 5)\n",
        "    noise_energy = np.mean(np.abs(audio_data)[np.abs(audio_data) < noise_threshold] ** 2)\n",
        "\n",
        "    snr = 10 * np.log10(energy / noise_energy)\n",
        "\n",
        "    if snr < 15:  # Minimum SNR of 15dB\n",
        "        return False, \"Too much background noise.\"\n",
        "    return True, \"Background noise level is acceptable.\"\n",
        "\n",
        "\n",
        "def convert_to_pcm(audio_path):\n",
        "    \"\"\"\n",
        "    Convert audio to 16-bit mono PCM with a sample rate of 16kHz.\n",
        "    \"\"\"\n",
        "    sound = AudioSegment.from_file(audio_path)\n",
        "    sound = sound.set_channels(1)  # Convert to mono\n",
        "    sound = sound.set_frame_rate(16000)  # Set sample rate to 16kHz\n",
        "    pcm_path = \"converted_audio.wav\"\n",
        "    sound.export(pcm_path, format=\"wav\")\n",
        "    return pcm_path\n",
        "\n",
        "\n",
        "def check_voice_presence(audio_path):\n",
        "    \"\"\"\n",
        "    Check if the audio contains human speech using webrtcvad.\n",
        "    \"\"\"\n",
        "    pcm_audio_path = convert_to_pcm(audio_path)\n",
        "\n",
        "    vad = webrtcvad.Vad()\n",
        "    vad.set_mode(1)  # Mode 1: more aggressive detection of speech\n",
        "\n",
        "    with contextlib.closing(wave.open(pcm_audio_path, 'rb')) as wf:\n",
        "        sample_rate = wf.getframerate()\n",
        "        assert sample_rate == 16000, \"Sample rate must be 16000 Hz\"\n",
        "        pcm_data = wf.readframes(wf.getnframes())\n",
        "\n",
        "        frame_duration = 30  # Frame size: 30ms\n",
        "        frame_size = int(sample_rate * frame_duration / 1000)\n",
        "\n",
        "        for i in range(0, len(pcm_data), frame_size * 2):  # 2 bytes per sample (16-bit audio)\n",
        "            frame = pcm_data[i:i + frame_size * 2]\n",
        "            if vad.is_speech(frame, sample_rate):\n",
        "                return True, \"Voice detected in the audio.\"\n",
        "\n",
        "    return False, \"No voice detected in the audio.\"\n",
        "\n",
        "\n",
        "def transcribe_audio(audio_path):\n",
        "    model = whisper.load_model(\"small\")\n",
        "    result = model.transcribe(audio_path)\n",
        "\n",
        "    transcription_text_file = \"transcription.txt\"\n",
        "    transcription_text = result['text']\n",
        "    with open(transcription_text_file, 'w') as f:\n",
        "        f.write(transcription_text)\n",
        "\n",
        "    print(\"Transcription saved to transcription.txt\")\n",
        "    return transcription_text_file"
      ],
      "metadata": {
        "id": "I_9v3nKXTOKV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "def extract_images_from_video(video_path, output_dir=\"frames\", interval=10):\n",
        "    \"\"\"\n",
        "    Extract frames from video every 'interval' seconds.\n",
        "    :param video_path: Path to the video file.\n",
        "    :param output_dir: Directory to save the extracted images.\n",
        "    :param interval: Time interval between frames to extract (in seconds).\n",
        "    \"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)  # Frames per second\n",
        "    frame_interval = int(fps * interval)\n",
        "\n",
        "    count = 0\n",
        "    success = True\n",
        "    while success:\n",
        "        success, frame = cap.read()\n",
        "        if count % frame_interval == 0 and success:\n",
        "            # Use zero-padding to ensure correct sorting\n",
        "            img_path = os.path.join(output_dir, f\"frame_{count:05d}.jpg\")\n",
        "            cv2.imwrite(img_path, frame)\n",
        "        count += 1\n",
        "\n",
        "    cap.release()\n",
        "    return output_dir"
      ],
      "metadata": {
        "id": "uE1hTkWmTOH0"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import easyocr\n",
        "import whisper\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "import numpy as np\n",
        "\n",
        "def is_unique_image(img_path, unique_images, similarity_threshold=0.95):\n",
        "    \"\"\"\n",
        "    Check if the image is unique compared to already processed unique images.\n",
        "    :param img_path: Path to the image file.\n",
        "    :param unique_images: List of already processed unique images.\n",
        "    :param similarity_threshold: Similarity threshold for uniqueness.\n",
        "    :return: True if the image is unique, False otherwise.\n",
        "    \"\"\"\n",
        "    img = cv2.imread(img_path)\n",
        "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    for unique_img in unique_images:\n",
        "        unique_img_gray = cv2.cvtColor(unique_img, cv2.COLOR_BGR2GRAY)\n",
        "        # Calculate the Structural Similarity Index (SSI)\n",
        "        score = ssim(img_gray, unique_img_gray)\n",
        "        if score >= similarity_threshold:\n",
        "            return False  # Image is similar to an already processed unique image\n",
        "\n",
        "    return True  # Image is unique\n",
        "\n",
        "\n",
        "def extract_text_from_image(img_path):\n",
        "    \"\"\"\n",
        "    Extract text from an image using EasyOCR. Skip if no text is detected.\n",
        "    :param img_path: Path to the image file.\n",
        "    :return: Extracted text or None if no text is detected.\n",
        "    \"\"\"\n",
        "    reader = easyocr.Reader(['en'])\n",
        "    results = reader.readtext(img_path)\n",
        "\n",
        "    if not results:  # No text detected\n",
        "        print(f\"No text detected in image: {img_path}\")\n",
        "        return None\n",
        "\n",
        "    extracted_text = \" \".join([text[1] for text in results])  # Concatenate all detected text\n",
        "    return extracted_text\n",
        "\n",
        "def analyze_frames(frames_dir, similarity_threshold=0.95):\n",
        "    \"\"\"\n",
        "    Analyze frames in a directory to extract text from unique images.\n",
        "    :param frames_dir: Directory containing the frames.\n",
        "    :param similarity_threshold: Threshold for image similarity.\n",
        "    :return: Dictionary of extracted texts from unique images.\n",
        "    \"\"\"\n",
        "    unique_images = []\n",
        "    extracted_texts = {}\n",
        "\n",
        "    for image_file in os.listdir(frames_dir):\n",
        "        img_path = os.path.join(frames_dir, image_file)\n",
        "        if is_unique_image(img_path, unique_images, similarity_threshold):\n",
        "            # If unique, read the image and extract text\n",
        "            extracted_text = extract_text_from_image(img_path)\n",
        "            extracted_texts[image_file] = extracted_text\n",
        "            # Store the image for future uniqueness checks\n",
        "            unique_images.append(cv2.imread(img_path))\n",
        "\n",
        "    return extracted_texts\n",
        "\n",
        "def transcribe_audio(audio_path):\n",
        "    \"\"\"\n",
        "    Transcribe audio using Whisper model.\n",
        "    :param audio_path: Path to the audio file.\n",
        "    :return: Transcribed text.\n",
        "    \"\"\"\n",
        "    model = whisper.load_model(\"small\")\n",
        "    result = model.transcribe(audio_path)\n",
        "    return result['text']\n",
        "\n",
        "def text_file(video_file):\n",
        "\n",
        "    # Extract text from frames and transcribe audio\n",
        "    extracted_texts = analyze_frames(frames_directory)\n",
        "    audio_text = transcribe_audio(audio_file)\n",
        "\n",
        "    # Combine texts\n",
        "    combined_text = \"Extracted Texts from Frames:\\n\"\n",
        "    for img_name, text in extracted_texts.items():\n",
        "        combined_text += f\"Extracted Text from :\\n{text}\\n\\n\"\n",
        "\n",
        "    combined_text += \"Transcribed Audio Text:\\n\"\n",
        "    combined_text += audio_text\n",
        "\n",
        "    # Save to a text file\n",
        "    output_file_path = 'extracted_texts_combined.txt'\n",
        "    with open(output_file_path, 'w') as file:\n",
        "        file.write(combined_text)\n",
        "\n",
        "    return output_file_path"
      ],
      "metadata": {
        "id": "89jios9hTOFk"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "def verification_report():\n",
        "    # Set the TogetherAI API key\n",
        "    api_key = \"0b79e2f0ddb16654bc98df9f828e0474d53c7d00eac41328abf06bd4858d14bb\"\n",
        "    #       combined_text = text_file()\n",
        "    # Load the transcribed text from the file\n",
        "    with open(combined_text_file, \"r\") as file:\n",
        "        transcript = file.read()\n",
        "\n",
        "    # Create a more detailed and structured prompt for verification task\n",
        "    prompt = f\"\"\"\n",
        "    You are tasked with analyzing the transcription of a business model canvas (BMC) presentation video.\n",
        "    Please perform the following checks and provide the results in JSON format:\n",
        "\n",
        "    1. **Names:** Extract the names of students mentioned when they introduce themselves.\n",
        "    2. **Introduction:** Check if there is a formal introduction at the beginning of the transcription. Indicate 'Yes' if an introduction is present and 'No' if not.\n",
        "    3. **Relevance:** Verify if the content of the transcription is relevant to the video title '{title}'. Answer 'Yes' or 'No' and explain briefly why the content is or is not relevant.\n",
        "    4. **BMC Topics:** Confirm whether the transcription covers all 9 BMC topics: key partners, key activities, value propositions, customer relationships, customer segments, key resources, channels, cost structure, and revenue streams. Answer 'Yes' or 'No' and list the missing topics if any.\n",
        "\n",
        "    Transcription to analyze:\n",
        "    {transcript}\n",
        "\n",
        "    Please provide your analysis in the following JSON format:\n",
        "\n",
        "    {{\n",
        "        \"Names\": [\"List of names\"],\n",
        "        \"Introduction\": \"Yes/No\",\n",
        "        \"Relevance\": {{\n",
        "            \"IsRelevant\": \"Yes/No\",\n",
        "            \"Reason\": \"Brief explanation of relevance\"\n",
        "        }},\n",
        "        \"BMC Topics\": {{\n",
        "            \"AllTopicsCovered\": \"Yes/No\",\n",
        "            \"MissingTopics\": [\"List of missing topics, if any\"]\n",
        "        }}\n",
        "    }}\n",
        "    \"\"\"\n",
        "\n",
        "    # API endpoint\n",
        "    url = \"https://api.together.xyz/v1/chat/completions\"\n",
        "\n",
        "    # Headers\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {api_key}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    # Request payload\n",
        "    data = {\n",
        "        \"model\": \"meta-llama/Llama-Vision-Free\",\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
        "    }\n",
        "\n",
        "    # Send the request to TogetherAI API\n",
        "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
        "\n",
        "    # Check if the request was successful\n",
        "    if response.status_code == 200:\n",
        "        # Parse the response\n",
        "        response_content = response.json()\n",
        "        # Extract the relevant message from the response\n",
        "        message_content = response_content[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "        # Print the raw response\n",
        "        print(\"Raw Response:\", message_content)\n",
        "\n",
        "        # Try to parse it as JSON\n",
        "        try:\n",
        "            response_data = json.loads(message_content)  # Load the response as JSON\n",
        "            print(\"Parsed Response:\", json.dumps(response_data, indent=2))  # Pretty print the parsed JSON\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(\"Failed to decode JSON response:\", e)\n",
        "    else:\n",
        "        print(f\"Request failed with status code {response.status_code}: {response.text}\")"
      ],
      "metadata": {
        "id": "hV4MX5OjTOC0"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main code block\n",
        "video_url = 'https://youtu.be/YTvjb_j36DI?si=0bTsYuuC35lFxnZu'\n",
        "title = get_video_info(video_url)\n",
        "video_file = download_video(video_url)\n",
        "if video_file:\n",
        "    print(f\"Video downloaded successfully: {video_file}\")\n",
        "    print(f\"Video title: {title}\")\n",
        "\n",
        "    quality_check, quality_report = check_video_quality(video_file)\n",
        "    print(quality_report)\n",
        "\n",
        "    if quality_check:\n",
        "        audio_file = extract_audio(video_file)\n",
        "        if audio_file:\n",
        "            audio_checks = [\n",
        "                check_audio_quality(audio_file),\n",
        "                check_audio_tone(audio_file),\n",
        "                check_background_noise(audio_file),\n",
        "                check_voice_presence(audio_file),\n",
        "            ]\n",
        "\n",
        "            if all(check[0] for check in audio_checks):\n",
        "                # Call the text_file() function to proceed with text extraction and verification\n",
        "                frames_directory = extract_images_from_video(video_file, interval=15)\n",
        "\n",
        "                combined_text_file = text_file(video_file)\n",
        "                print(f\"Combined text file created: {combined_text_file}\")\n",
        "                if combined_text_file:\n",
        "                    # Generate the verification report\n",
        "                    report = verification_report()\n",
        "                    print(report)\n",
        "                else:\n",
        "                  print('Failed to combine the text')\n",
        "            else:\n",
        "                for check in audio_checks:\n",
        "                    if not check[0]:\n",
        "                        print(check[1])\n",
        "\n",
        "        else:\n",
        "            print(\"Audio extraction failed.\")\n",
        "    else:\n",
        "        print(\"Audio extraction skipped due to low video quality.\")\n",
        "else:\n",
        "    print(\"Failed to download the video.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvBVERYcTOAM",
        "outputId": "3ba1bbe7-8208-4ac8-f321-edc7a381964b",
        "collapsed": true
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] Extracting URL: https://youtu.be/YTvjb_j36DI?si=0bTsYuuC35lFxnZu\n",
            "[youtube] YTvjb_j36DI: Downloading webpage\n",
            "[youtube] YTvjb_j36DI: Downloading ios player API JSON\n",
            "[youtube] YTvjb_j36DI: Downloading mweb player API JSON\n",
            "[youtube] YTvjb_j36DI: Downloading player 606a66b3\n",
            "[youtube] YTvjb_j36DI: Downloading m3u8 information\n",
            "[youtube] Extracting URL: https://youtu.be/YTvjb_j36DI?si=0bTsYuuC35lFxnZu\n",
            "[youtube] YTvjb_j36DI: Downloading webpage\n",
            "[youtube] YTvjb_j36DI: Downloading ios player API JSON\n",
            "[youtube] YTvjb_j36DI: Downloading mweb player API JSON\n",
            "[youtube] YTvjb_j36DI: Downloading m3u8 information\n",
            "[info] YTvjb_j36DI: Downloading 1 format(s): 136+140\n",
            "[download] Destination: video.mp4.f136.mp4\n",
            "[download] 100% of    4.93MiB in 00:00:00 at 19.98MiB/s  \n",
            "[download] Destination: video.mp4.f140.m4a\n",
            "[download] 100% of    3.41MiB in 00:00:00 at 19.79MiB/s  \n",
            "[Merger] Merging formats into \"video.mp4.mkv\"\n",
            "Deleting original file video.mp4.f136.mp4 (pass -k to keep)\n",
            "Deleting original file video.mp4.f140.m4a (pass -k to keep)\n",
            "Video downloaded successfully: video.mp4.mkv\n",
            "Video title: tencent -\n",
            "Video is valid: 1280x720 resolution.\n",
            "Executing command: ffmpeg -i video.mp4.mkv -q:a 0 -map a audio.mp3\n",
            "Audio extracted successfully: audio.mp3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:easyocr.easyocr:Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
            "WARNING:easyocr.easyocr:Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
            "WARNING:easyocr.easyocr:Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
            "WARNING:easyocr.easyocr:Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
            "WARNING:easyocr.easyocr:Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
            "/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n",
            "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined text file created: extracted_texts_combined.txt\n",
            "Raw Response: Here is the analysis in the requested JSON format:\n",
            "\n",
            "```json\n",
            "{\n",
            "    \"Names\": [\"Srinan\", \"SRI RAM\"],\n",
            "    \"Introduction\": \"Yes\",\n",
            "    \"Relevance\": {\n",
            "        \"IsRelevant\": \"Yes\",\n",
            "        \"Reason\": \"The transcription is about the business model canvas of Tencent, which matches the video title 'tencent -'.\"\n",
            "    },\n",
            "    \"BMC Topics\": {\n",
            "        \"AllTopicsCovered\": \"Yes\"\n",
            "    }\n",
            "}\n",
            "```\n",
            "\n",
            "Explanation:\n",
            "\n",
            "1. **Names:** The transcription mentions two names: Srinan and SRI RAM. These are the students who introduced themselves at the beginning of the video.\n",
            "2. **Introduction:** There is a formal introduction at the beginning of the transcription, where Srinan introduces himself and explains the context of the video. Therefore, the answer is \"Yes\".\n",
            "3. **Relevance:** The transcription is about the business model canvas of Tencent, which matches the video title 'tencent -'. The content is relevant to the title, and the transcription provides a detailed explanation of Tencent's business model. Therefore, the answer is \"Yes\".\n",
            "4. **BMC Topics:** After reviewing the transcription, I can confirm that all 9 BMC topics are covered:\n",
            "\t* Key partners: Chinese Government and App Developers, OS brand\n",
            "\t* Key activities: design and executing the products, roadmap, managing and maintaining the platform\n",
            "\t* Value propositions: super app to meet every online and offline needs of the internet users\n",
            "\t* Customer relationships: Communication and Self-Service and Co-Creativity Channels\n",
            "\t* Customer segments: internet users and social media advertisements\n",
            "\t* Key resources: web chat platform, Service Forms, Big Data Analysis\n",
            "\t* Channels: chat app for communication, customer segment, marketing\n",
            "\t* Cost structure: IT system, legal marketing, taxes\n",
            "\t* Revenue streams: social media advertising added as service\n",
            "\n",
            "All topics are covered, and therefore, the answer is \"Yes\".\n",
            "Failed to decode JSON response: Expecting value: line 1 column 1 (char 0)\n",
            "None\n"
          ]
        }
      ]
    }
  ]
}